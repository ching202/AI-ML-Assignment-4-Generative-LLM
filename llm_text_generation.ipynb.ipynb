{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddeb7f8f-f385-4675-86f5-7e3027e7c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (4.57.2)\n",
      "Requirement already satisfied: torch in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ky3pe\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d08cc6-8016-4cc5-a275-d1751eb3da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.2\n",
      "Torch version: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Transformers version:\", __import__(\"transformers\").__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bbab50-70fa-4c49-a228-15eb39118321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded: openai-community/gpt2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"openai-community/gpt2\"  # small GPT-2 model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c645f904-c8da-4bb0-a207-059c08f14f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 50,\n",
    "    top_p: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from GPT-2 given a prompt and generation parameters.\n",
    "    \"\"\"\n",
    "    # 1. Encode the prompt into input IDs\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # 2. Use model.generate with our chosen parameters\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,  # how long the continuation can be\n",
    "            temperature=temperature,        # controls randomness\n",
    "            top_p=top_p,                    # nucleus sampling\n",
    "            do_sample=True,                 # sampling instead of greedy\n",
    "        )\n",
    "\n",
    "    # 3. Decode the output IDs back to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f11d83-c130-4a24-9081-a6935604d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in Washington D.C., a young developer decided to learn about AI. He was inspired by the success of AI startups such as Amazon and Google, and began working on a product that would allow the company to develop its own AI.\n",
      "\n",
      "It's an interesting development, but it's not a new one. For a while, AI was considered to be a niche area\n"
     ]
    }
   ],
   "source": [
    "#Testing Above output\n",
    "prompt = \"Once upon a time in Washington D.C., a young developer decided to learn about AI.\"\n",
    "result = generate_text(prompt, temperature=0.7, max_new_tokens=60, top_p=0.9)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a500c01-37da-497f-b5bd-f59c773ab40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in Washington D.C., a young developer decided to learn about AI. The company's founder and President Javi Kurie told me this time last January when, looking under the hood of Windows Vista Linux and Windows 7 OS systems, IBM bought that developer companyâ€”not Mac or Google in the former; instead Intel had to \"develop every computer and the latest product every half-billion year, a computer with the greatest future plans on one giant box in Washington Washington DC with\n"
     ]
    }
   ],
   "source": [
    "#Test Case 2 with different parameters\n",
    "prompt = \"Once upon a time in Washington D.C., a young developer decided to learn about AI.\"\n",
    "result = generate_text(prompt, temperature=2.0, max_new_tokens=80, top_p=2.9)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bab4ba2-9eb0-4a5b-939a-33efa65d6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an aspiring software engineer learning about large language models. Write a short, imaginative story about how training an AI model helped you start a new chapter in your life.\n"
     ]
    }
   ],
   "source": [
    "#The base prompt to continue from\n",
    "base_prompt = (\n",
    "    \"You are an aspiring software engineer learning about large language models. \"\n",
    "    \"Write a short, imaginative story about how training an AI model helped you \"\n",
    "    \"start a new chapter in your life.\"\n",
    ")\n",
    "print(base_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ac193c-9bd2-41da-9daf-c67f165f0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Temperature = 0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an aspiring software engineer learning about large language models. Write a short, imaginative story about how training an AI model helped you start a new chapter in your life.\n",
      "\n",
      "The best way to learn about large language models is to read the book.\n",
      "\n",
      "The book is a great resource for learning about large language models. It is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The book is a good place to start.\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Temperature = 0.7\n",
      "================================================================================\n",
      "You are an aspiring software engineer learning about large language models. Write a short, imaginative story about how training an AI model helped you start a new chapter in your life.\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Temperature = 1.0\n",
      "================================================================================\n",
      "You are an aspiring software engineer learning about large language models. Write a short, imaginative story about how training an AI model helped you start a new chapter in your life. You will also learn about the role of social capital in AI technology.\n",
      "\n",
      "This course will teach you about how to use social capital to build and optimize social models.\n",
      "\n",
      "Course Overview\n",
      "\n",
      "The course introduces the concept of machine learning. We'll learn to use AI to build and fine-tune AI models of complex language models and how to apply those concepts to language models. We will use machine learning to analyze your language model and help you decide which language is the best model for your job. The courses a\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.2, 0.7, 1.0]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    text = generate_text(\n",
    "        prompt=base_prompt,\n",
    "        temperature=temp,\n",
    "        max_new_tokens=120,  # a bit longer to see style differences\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Save the text so we can use it later for README table\n",
    "    results[temp] = text\n",
    "\n",
    "    # Print just part of it so the cell output isn't crazy long\n",
    "    print(text[:700])  # first 700 characters\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce230760-a7a1-46ec-9b1e-dc6ba1f55f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b02de9-dfe5-4d70-b8dd-62dbd11a8c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
